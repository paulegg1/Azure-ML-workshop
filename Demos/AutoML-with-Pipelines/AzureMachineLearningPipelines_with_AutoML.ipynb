{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Azure Machine Learning Pipelines with Auto ML\n",
        "In this demonstration, we will be looking at how ot discover high performing models with Azure Machine learning pipelines.\n",
        "This demonstration is adapted from AutoML pipeline to fit the scenario for this activate title regression\n",
        "1. Use automated ML in an Azure Machine Learning pipeline in Python - https://docs.microsoft.com/en-us/azure/machine-learning/how-to-use-automlstep-in-pipelines"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Referencing Machine Learning workspace from config file created in previous steps\n",
        "In this step we are getting details of machine learning workspace previously created from the config file"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### The below cell can be executed if you are running the notebook locally in this machine and you created the workspace using the portal. Replace subscription-id, resource-group and workspace-name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1685877735354
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from azureml.core import Workspace\n",
        "\n",
        "#subscription_id = '<subscription_id>'\n",
        "#resource_group  = '<resource_group>'\n",
        "#workspace_name  = '<workspace_name>'\n",
        "\n",
        "#try:\n",
        "#    ws = Workspace(subscription_id = subscription_id, resource_group = resource_group, workspace_name = workspace_name)\n",
        "#    ws.write_config()\n",
        "#    print('Library configuration succeeded')\n",
        "#except:\n",
        "#    print('Workspace not found')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1685877736452
        }
      },
      "outputs": [],
      "source": [
        "import azureml.core\n",
        "from azureml.core import Workspace, Datastore\n",
        "\n",
        "ws = Workspace.from_config()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1685877737629
        }
      },
      "outputs": [],
      "source": [
        "ws.get_details()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Getting Datastore, Blobstore and Filestore in the workspace\n",
        "In this step, we will define the datastore, blobstore and file store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1685877738766
        }
      },
      "outputs": [],
      "source": [
        "# Default datastore \n",
        "def_data_store = ws.get_default_datastore()\n",
        "\n",
        "# Get the blob storage associated with the workspace\n",
        "def_blob_store = Datastore(ws, \"workspaceblobstore\")\n",
        "\n",
        "# Get file storage associated with the workspace\n",
        "def_file_store = Datastore(ws, \"workspacefilestore\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Examine the data store details"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1685877740043
        }
      },
      "outputs": [],
      "source": [
        "def_data_store"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Upload dataset and register the dataset in the workspace\n",
        "In the following steps, we will upload the training and test dataset in the workspace blobstore and create a dataset that can be used further in the pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1685877741196
        }
      },
      "outputs": [],
      "source": [
        "def_blob_store.upload_files(\n",
        "    [\"./LengthOfStay_trimmed.csv\"],\n",
        "    target_path=\"train-dataset\",\n",
        "    overwrite=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        " If there isn't already a dataset named 'patient_los_dataset' registered, then it creates one. The code downloads CSV data from the Web, uses them to instantiate a TabularDataset and then registers the dataset with the workspace. Finally, the function Dataset.get_by_name() assigns the Dataset to patient_los_dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1685877745642
        }
      },
      "outputs": [],
      "source": [
        "from azureml.core import Dataset\n",
        "ws = Workspace.from_config()\n",
        "datastore = Datastore.get(ws, 'workspaceblobstore')\n",
        "patient_los_dataset = Dataset.Tabular.from_delimited_files([(datastore, 'train-dataset/LengthOfStay_trimmed.csv')])\n",
        "patient_los_dataset.register(workspace = ws,\n",
        "                                     name = 'patient_los_dataset',\n",
        "                                     description = 'patient los data',\n",
        "                                     create_new_version = True)\n",
        "\n",
        "patient_los_dataset = Dataset.get_by_name(ws, 'patient_los_dataset')\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "The below code fetches dataset keys Vire which provides information about the dataset registration and version and any tags that may have been used while registering the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1685877746513
        }
      },
      "outputs": [],
      "source": [
        "ws.datasets.keys()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create compute cluster\n",
        "In the step below, we will create a compute target to run the pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1685877747884
        }
      },
      "outputs": [],
      "source": [
        "from azureml.core import Datastore\n",
        "from azureml.core.compute import AmlCompute, ComputeTarget\n",
        "\n",
        "datastore = ws.get_default_datastore()\n",
        "\n",
        "compute_name = 'cpu-cluster'\n",
        "if not compute_name in ws.compute_targets :\n",
        "    print('creating a new compute target...')\n",
        "    provisioning_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D2_V2',\n",
        "                                                                max_nodes=4)\n",
        "    compute_target = ComputeTarget.create(ws, compute_name, provisioning_config)\n",
        "\n",
        "    compute_target.wait_for_completion(\n",
        "        show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
        "\n",
        "    # Show the result\n",
        "    print(compute_target.get_status().serialize())\n",
        "\n",
        "compute_target = ws.compute_targets[compute_name]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "The below code prints metadata about the patient los dataset that we uploaded in the earlier steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1685877748789
        }
      },
      "outputs": [],
      "source": [
        "print(patient_los_dataset)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configure the training run\n",
        "This step is to make sure that the remote training run has all the dependencies that are required by the training steps. Dependencies and the runtime context are set by creating and configuring a RunConfiguration object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1685877749504
        }
      },
      "outputs": [],
      "source": [
        "#!pip install ruamel.yaml==0.17.4 --user"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1685877750409
        }
      },
      "outputs": [],
      "source": [
        "#``ruamel.yaml<=0.15``"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "This step configures the training run.The runtime context is set by creating and configuring a RunConfiguration object. Here we set the compute target created earlier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1685877751267
        }
      },
      "outputs": [],
      "source": [
        "from azureml.core.runconfig import RunConfiguration\n",
        "from azureml.core.conda_dependencies import CondaDependencies\n",
        "from azureml.core import Environment \n",
        "\n",
        "aml_run_config = RunConfiguration()\n",
        "# Use just-specified compute target (\"cpu-cluster\")\n",
        "aml_run_config.target = compute_target\n",
        "\n",
        "USE_CURATED_ENV = False\n",
        "if USE_CURATED_ENV :\n",
        "    curated_environment = Environment.get(workspace=ws, name=\"AzureML-Tutorial\")\n",
        "    aml_run_config.environment = curated_environment\n",
        "else:\n",
        "    aml_run_config.environment.python.user_managed_dependencies = False\n",
        "    \n",
        "    # Add some packages relied on by data prep step\n",
        "    aml_run_config.environment.python.conda_dependencies = CondaDependencies.create(\n",
        "        conda_packages=['pandas','scikit-learn'], \n",
        "        pip_packages=['azureml-sdk[automl,explain]', 'azureml-dataprep[fuse,pandas]'], \n",
        "        pin_sdk_version=False)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preparing data for Auto ML regression\n",
        "In this step, we are doing data preparation to drop columns that wont be used for prediction. This can be extended further to do complete data preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "microsoft": {
          "language": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "%%writefile dataprep.py\n",
        "from azureml.core import Run\n",
        "\n",
        "import pandas as pd \n",
        "import numpy as np \n",
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq\n",
        "import argparse\n",
        "RANDOM_SEED=42\n",
        "def prepare_train_x(df):\n",
        "    # drop the predicted values of the dataset that relates to classification \n",
        "    train_x = df.drop(['vdate'], axis=1)\n",
        "    return train_x\n",
        "\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--output_path', dest='output_path', required=True)\n",
        "args = parser.parse_args()\n",
        "patient_los_dataset = Run.get_context().input_datasets['patient_los_dataset']\n",
        "\n",
        "df_train = patient_los_dataset.to_pandas_dataframe()\n",
        "\n",
        "prepare_train_x_df=prepare_train_x(df_train)\n",
        "\n",
        "os.makedirs(os.path.dirname(args.output_path), exist_ok=True)\n",
        "pq.write_table(pa.Table.from_pandas(prepare_train_x_df), args.output_path)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(f\"Wrote test to {args.output_path} and train to {args.output_path}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define Data preparation step for pipeline\n",
        "In this step we are defining data preparation step with the python file created earlier for data preparation"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "The data preparation code described above must be associated with a PythonScripStep object to be used with a pipeline. The path to which the CSV output is written is generated by a OutputFileDatasetConfig object. The resources prepared earlier, such as the ComputeTarget, the RunConfig, and the 'patient_los_dataset' Dataset are used to complete the specification.\n",
        "The prepped_data_path object is of type OutputFileDatasetConfig which points to a directory. Notice that it's specified in the arguments parameter. If you review the previous step, you'll see that within the data preparation code, the value of the argument '--output_path' is the directory path at which the CSV file was written."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1685877752611
        }
      },
      "outputs": [],
      "source": [
        "from azureml.pipeline.core import PipelineData,  InputPortBinding, Pipeline\n",
        "from azureml.pipeline.steps import PythonScriptStep\n",
        "\n",
        "#datastore = Datastore.get(ws, 'workspaceblobstore')\n",
        "\n",
        "prepped_data_path = PipelineData(\"patient_los_dataset\",def_data_store,\"direct\").as_dataset()\n",
        "\n",
        "\n",
        "dataprep_step = PythonScriptStep(\n",
        "    name=\"dataprep\", \n",
        "    script_name=\"dataprep.py\", \n",
        "    compute_target=compute_target, \n",
        "    runconfig=aml_run_config,\n",
        "    arguments=[\"--output_path\", prepped_data_path],\n",
        "    inputs=[patient_los_dataset.as_named_input(\"patient_los_dataset\")],\n",
        "    outputs=[prepped_data_path],\n",
        "    allow_reuse=True\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Train with AutoMLStep"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Configuring an automated ML pipeline step is done with the AutoMLConfig class. This flexible class is described in Configure automated ML experiments in Python. Data input and output are the only aspects of configuration that require special attention in an ML pipeline. Input and output for AutoMLConfig in pipelines is discussed in detail below. Beyond data, an advantage of ML pipelines is the ability to use different compute targets for different steps. You might choose to use a more powerful ComputeTarget only for the automated ML process. Doing so is as straightforward as assigning a more powerful RunConfiguration to the AutoMLConfig object's run_configuration parameter."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Send data to AutoML Step\n",
        "The snippet below creates a high-performing PipelineOutputTabularDataset from the PipelineOutputFileDataset output of the data preparation step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1685877753471
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "prepped_data_path\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1685877754303
        }
      },
      "outputs": [],
      "source": [
        "# type(prepped_data_path) == PipelineOutputFileDataset\n",
        "# type(prepped_data) == PipelineOutputTabularDataset\n",
        "prepped_data = prepped_data_path.parse_parquet_files(file_extension=None)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Specify Automated ML Outputs\n",
        "The outputs of the AutoMLStep are the final metric scores of the higher-performing model and that model itself. To use these outputs in further pipeline steps, prepare PipelineData objects to receive them.The snippet above creates the two PipelineData objects for the metrics and model output. Each is named, assigned to the default datastore retrieved earlier, and associated with the particular type of TrainingOutput from the AutoMLStep. Because we assign pipeline_output_name on these PipelineData objects, their values will be available not just from the individual pipeline step, but from the pipeline as a whole"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1685877755580
        }
      },
      "outputs": [],
      "source": [
        "from azureml.pipeline.core import TrainingOutput\n",
        "\n",
        "metrics_data = PipelineData(name='metrics_data',\n",
        "                           datastore=datastore,\n",
        "                           pipeline_output_name='metrics_output',\n",
        "                           training_output=TrainingOutput(type='Metrics'))\n",
        "model_data = PipelineData(name='best_model_data',\n",
        "                           datastore=datastore,\n",
        "                           pipeline_output_name='model_output',\n",
        "                           training_output=TrainingOutput(type='Model'))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configure and Create Automated ML Pipeline Step\n",
        "Once the inputs and outputs are defined, it's time to create the AutoMLConfig and AutoMLStep. The details of the configuration will depend on your task, in this case, it is regression to predict los from the patient los Dataset.\n",
        "The automl_settings dictionary is passed to the AutoMLConfig constructor as kwargs. The other parameters aren't complex:\n",
        "\n",
        "- task is set to regression for this example. Other valid values are classification and forecasting\n",
        "- path and debug_log describe the path to the project and a local file to which debug information will be written\n",
        "- compute_target is the previously defined compute_target that, in this example, is an inexpensive CPU-based machine. If you're using AutoML's Deep Learning facilities, you would want to change the compute target to be GPU-based\n",
        "- featurization is set to auto. Indicates that as part of preprocessing, data guardrails and featurization steps are performed automatically. This is the default option.\n",
        "- label_column_name indicates which column we are interested in predicting\n",
        "- training_data is set to the OutputTabularDatasetConfig objects made from the outputs of the data preparation step\n",
        "\n",
        "The AutoMLStep itself takes the AutoMLConfig and has, as outputs, the PipelineData objects created to hold the metrics and model data\n",
        "\n",
        "In this example, the automated ML process will perform cross-validations on the training_data. You can control the number of cross-validations with the n_cross_validations argument. If you've already split your training data as part of your data preparation steps, you can set validation_data to its own Dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1685877756832
        }
      },
      "outputs": [],
      "source": [
        "from azureml.train.automl import AutoMLConfig\n",
        "from azureml.pipeline.steps import AutoMLStep\n",
        "import logging\n",
        "automl_settings = {\n",
        "       \"n_cross_validations\":5,\n",
        "       \"primary_metric\": 'r2_score',\n",
        "       \"enable_early_stopping\": True,\n",
        "       #change the timeout for shorter run of the experiment if there is no time to demonstrate\n",
        "       \"experiment_timeout_hours\": 1.0,\n",
        "       \"max_concurrent_iterations\": 4,\n",
        "       \"max_cores_per_iteration\": -1,\n",
        "       \"verbosity\": logging.INFO\n",
        "   }\n",
        "\n",
        "automl_config = AutoMLConfig(task = 'regression',\n",
        "                               path = '.',\n",
        "                               compute_target = compute_target,\n",
        "                               training_data = prepped_data,\n",
        "                           #    run_configuration = aml_run_config,\n",
        "                               featurization = 'auto',\n",
        "                               debug_log = 'automated_ml_errors.log',\n",
        "                               label_column_name = 'lengthofstay',\n",
        "                               **automl_settings\n",
        "                               )\n",
        "\n",
        "train_step = AutoMLStep(name='AutoMLregression',\n",
        "    automl_config=automl_config,\n",
        "    passthru_automl_config=False,\n",
        "    outputs=[metrics_data,model_data],\n",
        "    allow_reuse=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Register the model created by automated ML\n",
        "The last step in a basic ML pipeline is registering the created model. By adding the model to the workspace's model registry, it will be available in the portal and can be versioned. To register the model, write another PythonScriptStep that takes the model_data output of the AutoMLStep(first and the second cell below this cell performs these steps).\n",
        "\n",
        "A model is registered in a Workspace. You're probably familiar with using Workspace.from_config() to log on to your workspace on your local machine, but there's another way to get the workspace from within a running ML pipeline. The Run.get_context() retrieves the active Run. This run object provides access to many important objects, including the Workspace used here.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "microsoft": {
          "language": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "%%writefile register_model.py\n",
        "from azureml.core.model import Model, Dataset\n",
        "from azureml.core.run import Run, _OfflineRun\n",
        "from azureml.core import Workspace\n",
        "import argparse\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--model_name\", required=True)\n",
        "parser.add_argument(\"--model_path\", required=True)\n",
        "args = parser.parse_args()\n",
        "\n",
        "print(f\"model_name : {args.model_name}\")\n",
        "print(f\"model_path: {args.model_path}\")\n",
        "\n",
        "run = Run.get_context()\n",
        "ws = Workspace.from_config() if type(run) == _OfflineRun else run.experiment.workspace\n",
        "\n",
        "model = Model.register(workspace=ws,\n",
        "                       model_path=args.model_path,\n",
        "                       model_name=args.model_name)\n",
        "\n",
        "print(\"Registered version {0} of model {1}\".format(model.version, model.name))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "The model-registering PythonScriptStep uses a PipelineParameter for one of its arguments. Pipeline parameters are arguments to pipelines that can be easily set at run-submission time. Once declared, they're passed as normal arguments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1685877758824
        }
      },
      "outputs": [],
      "source": [
        "from azureml.pipeline.core.graph import PipelineParameter\n",
        "\n",
        "# The model name with which to register the trained model in the workspace.\n",
        "model_name = PipelineParameter(\"model_name\", default_value=\"LOSPredict\")\n",
        "\n",
        "register_step = PythonScriptStep(script_name=\"register_model.py\",\n",
        "                                       name=\"register_model\",\n",
        "                                       allow_reuse=False,\n",
        "                                       arguments=[\"--model_name\", model_name, \"--model_path\", model_data],\n",
        "                                       inputs=[model_data],\n",
        "                                       compute_target=compute_target,\n",
        "                                       runconfig=aml_run_config)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create and run the automated ML pipeline\n",
        "Creating and running a pipeline that contains the AutoML Step\n",
        "\n",
        "The code below combines the data preparation, automated ML, and model-registering steps into a Pipeline object. It then creates an Experiment object. The Experiment constructor will retrieve the named experiment if it exists or create it if necessary. It submits the Pipeline to the Experiment, creating a Run object that will asynchronously run the pipeline. The wait_for_completion() function blocks until the run completes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1685876585263
        }
      },
      "outputs": [],
      "source": [
        "from azureml.pipeline.core import Pipeline\n",
        "from azureml.core import Experiment\n",
        "azureml._restclient.snapshots_client.SNAPSHOT_MAX_SIZE_BYTES = 20000000000\n",
        "\n",
        "\n",
        "pipeline = Pipeline(ws, [dataprep_step, train_step, register_step])\n",
        "#pipeline = Pipeline(ws, [dataprep_step,train_step])\n",
        "\n",
        "experiment = Experiment(workspace=ws, name='los_automl_pipeline')\n",
        "\n",
        "run = experiment.submit(pipeline, show_output=True)\n",
        "run.wait_for_completion()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Examine pipeline results\n",
        "Once the run completes, you can retrieve PipelineData objects that have been assigned a pipeline_output_name. You can download the results and load them for further processing. Downloaded files are written to the subdirectory azureml/{run.id}/. The metrics file is JSON-formatted and can be converted into a Pandas dataframe for examination."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1685875451339
        }
      },
      "outputs": [],
      "source": [
        "metrics_output_port = run.get_pipeline_output('metrics_output')\n",
        "model_output_port = run.get_pipeline_output('model_output')\n",
        "\n",
        "metrics_output_port.download('.', show_progress=True)\n",
        "model_output_port.download('.', show_progress=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Change the run_id in the metrics_file name parameter to the GUID like output you might get. For example, the code below uses this guid from metrics_data output above - azureml/ff426d72-94eb-4fa6-a42e-280c76e20d91/metrics_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1685875451433
        }
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "#metrics_filename = metrics_output._path_on_datastore\n",
        "metrics_filename = 'azureml/ff426d72-94eb-4fa6-a42e-280c76e20d91/metrics_data'\n",
        "with open(metrics_filename) as f:\n",
        "   metrics_output_result = f.read()\n",
        "   \n",
        "deserialized_metrics_output = json.loads(metrics_output_result)\n",
        "df = pd.DataFrame(deserialized_metrics_output)\n",
        "df"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This model file provided below can be further used for inferencing and metrics analysis and so on. Change the run_id portion in the model_filename parameter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1685875451505
        }
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import os\n",
        "#from sklearn.preprocessing import Imputer\n",
        "\n",
        "#from azureml.automl.core._shared_package_legacy_import import _import_all_legacy_submodules\n",
        "#model_filename = model_output._path_on_datastore\n",
        "#change the run id portion to the directory created under ParenDirectory/azureml\n",
        "model_filename =  'azureml/ff426d72-94eb-4fa6-a42e-280c76e20d91/best_model_data'\n",
        "\n",
        "with open(model_filename, \"rb\" ) as f:\n",
        "        best_model = pickle.load(f)\n",
        "       \n",
        "\n",
        "best_model"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "display_name": "Python 3.8 - AzureML",
      "language": "python",
      "name": "python38-azureml"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}
